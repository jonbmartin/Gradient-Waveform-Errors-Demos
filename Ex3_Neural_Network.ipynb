{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNil/ROMJCF3e2n6rFYRFTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonbmartin/Gradient-Waveform-Errors-Demos/blob/main/Ex3_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jonbmartin/Gradient-Waveform-Errors-Demos"
      ],
      "metadata": {
        "id": "VKInBfjT2_F7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f46e3e-f171-4282-c36f-8c2576ee772f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Gradient-Waveform-Errors-Demos'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 57 (delta 18), reused 17 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (57/57), 11.16 MiB | 7.66 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/Gradient-Waveform-Errors-Demos')\n",
        "sys.path.insert(0,'/content/Gradient-Waveform-Errors-Demos/utils/')"
      ],
      "metadata": {
        "id": "wRHia0JB4LyW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yp6olyJT5VRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "from belief_network_util import *\n",
        "from custom_environments_belief import *\n"
      ],
      "metadata": {
        "id": "cVWxeJw14RLa",
        "outputId": "e7fd160d-c35f-4796-f9c7-1ab8a48cc595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'util'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2996449431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbelief_network_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcustom_environments_belief\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Gradient-Waveform-Errors-Demos/utils/custom_environments_belief.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbelief_network_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_walk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure that all seeds are set for reproducibility\n",
        "seed = 12\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "XuSAZUaq4SSd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "data = sio.loadmat('Gradient-Waveform-Errors-Demos/data/all_ideal_waveforms_normalized.mat')\n",
        "g_ideal_m = np.squeeze(data['ideal'])\n",
        "\n",
        "gpu_id = -1\n",
        "axis = 'ax'\n",
        "\n",
        "belief_checkpoint_path = 'scanner_data_training_augmented_ampfeature_'+axis+'.pt'\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "vHmJ78HMmNDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#########################################\n",
        "#####\n",
        "# Training to initialize the belief network  #\n",
        "##############################################\n",
        "\n",
        "pretrain_belief_network = True\n",
        "plotting_belief_results = True\n",
        "return_traj_y = False\n",
        "input_size = 3 # (2 states and 1 action) - just gideal amplitude, precomp amp, action\n",
        "n_epochs = 150\n",
        "\n",
        "\n",
        "GMAX_CONSTRAINT = 275 # mT/m\n",
        "SMAX_CONSTRAINT = 100 # mT/m/s\n",
        "\n",
        "# set up which device we will be using\n",
        "device = torch.device('cuda:'+gpu_id if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device to be used = {device}')\n",
        "\n",
        "normalize = False\n",
        "buffer_length = int(50_000)\n",
        "\n",
        "# set up identity environment - the initial belief system, that pulse is undistorted\n",
        "initial_girf_env = GradEnvContBelief(g_ideal_m, scanner_girf, scanner_girf_t, gradient_model='girf',\n",
        "                                 normalize=normalize, gmax=GMAX_CONSTRAINT, smax=SMAX_CONSTRAINT, apply_constraint=False,\n",
        "                                 max_amplitude_range=[60, 120],)\n",
        "initial_girf_env.reset()\n",
        "\n",
        "# set up our TCN model! optimized hyperparameters:\n",
        "channel_sizes = [48] * 5 # temporal causal layer channels\n",
        "kernel_size = 16 # convolution kernel size\n",
        "dropout = 0.002\n",
        "batch_size = 32\n",
        "lr = 8.4E-04\n",
        "weight_decay = 8.7E-08\n",
        "eps = 4.24E-06\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999999\n",
        "window_size = 75\n",
        "predict_point = 65\n",
        "\n",
        "model_params = {\n",
        "    'input_size':   3,\n",
        "    'output_size':  1,\n",
        "    'num_channels': channel_sizes,\n",
        "    'kernel_size':  kernel_size,\n",
        "    'dropout':      dropout\n",
        "}\n",
        "belief_model = TCN(**model_params)\n",
        "belief_model.float() # TODO: this might cause things to go way more slowly (too precise)\n",
        "\n",
        "belief_training_buffer = BeliefWindowBuffer(initial_girf_env.observation_space, initial_girf_env.action_space,\n",
        "                                    buffer_length=buffer_length)\n",
        "\n",
        "# Loading the NORMALIZED data with feature scale\n",
        "if axis == 'ax':\n",
        "    file = 'scanner_data_gax_trainandtest_0424.csv'\n",
        "elif axis == 'sag':\n",
        "    file = 'scanner_data_gsag_trainandtest_0424.csv'\n",
        "elif axis == 'cor':\n",
        "    file = 'scanner_data_gcor_trainandtest_0424.csv'\n",
        "\n",
        "print('loading training/testing data from: '+ file)\n",
        "belief_training_buffer.add_transitions_from_csv(file)\n",
        "collected_observations = belief_training_buffer.get_observation_v_array()\n",
        "\n",
        "feature_indices = [0, 4] # input grad amp and waveform scale\n",
        "scale_factors = np.squeeze(collected_observations[:,4]) # normalized scale factors\n",
        "max_scale_mt_m = 275 # [mT/m], the normalizing max amplitude\n",
        "\n",
        "if plotting_belief_results:\n",
        "    plt.plot(collected_observations)\n",
        "    plt.legend(['g ideal', 's ideal', 'g precomp', 's precomp'])\n",
        "    plt.show()\n",
        "\n",
        "data_normalized_flag = True\n",
        "\n",
        "if pretrain_belief_network:\n",
        "\n",
        "    print(\"Status: Pretraining the belief model\")\n",
        "    train_test_split = 0.7603\n",
        "    train_size = int(len(belief_training_buffer.observed_error_v) * train_test_split)\n",
        "    print(f'Train size = {train_size}' )\n",
        "    X_train, X_test, y_train, y_test = belief_training_buffer.return_X_Y_train_test(train_test_split=train_test_split,\n",
        "                                             normalize=not data_normalized_flag, rl_env=initial_girf_env,\n",
        "                                             feature_indices=feature_indices)\n",
        "\n",
        "    X_train, y_train = create_dataset(X_train, y_train, window_size=window_size, predict_point=predict_point,\n",
        "                                      predict_single_timepoint=True,\n",
        "                                      pct_data_to_keep=1, return_traj_y=return_traj_y)\n",
        "    X_test, y_test = create_dataset(X_test, y_test, window_size=window_size, predict_point=predict_point,\n",
        "                                    predict_single_timepoint=True,\n",
        "                                    pct_data_to_keep=1, return_traj_y=return_traj_y)\n",
        "\n",
        "    X_train = X_train.transpose(1, 2)\n",
        "    X_test = X_test.transpose(1, 2)\n",
        "\n",
        "    # should be: [number of examples, data dimension, timeseries length (features)]\n",
        "    print(f'size of x train data = {X_train.size()}')\n",
        "    print(f'size of y train data = {y_train.size()}')\n",
        "    print(f'size of x test data = {X_test.size()}')\n",
        "    print(f'size of y test data = {y_test.size()}')\n",
        "\n",
        "    optimizer = optim.Adam(belief_model.parameters(),betas=(beta1, beta2), eps=eps,\n",
        "                           lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.L1Loss()\n",
        "\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "    loader = torch_data.DataLoader(torch_data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    belief_model, test_cost, best_params = train_and_test_belief_network(n_epochs, belief_model, loss_fn, optimizer, loader,\n",
        "                                            X_train, X_test, y_train, y_test, device=device)\n",
        "\n",
        "    print(f\"Status: Belief network pretraining complete, saving model at {belief_checkpoint_path}\")\n",
        "    checkpoint = {\n",
        "        'epoch': n_epochs,\n",
        "        'state_dict': best_params,\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "    save_checkpoint(checkpoint, belief_checkpoint_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "68nhAQ-a6_4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if plotting_belief_results:\n",
        "    print(\"Status: Plotting the initial results\")\n",
        "    train_test_split = 0.7603\n",
        "    train_size = int(len(belief_training_buffer.observed_error_v) * train_test_split)\n",
        "    print(f'Train size = {train_size}' )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = belief_training_buffer.return_X_Y_train_test(train_test_split=train_test_split, normalize=not data_normalized_flag,\n",
        "        rl_env=initial_girf_env, feature_indices=feature_indices)\n",
        "\n",
        "\n",
        "    X_train, y_train = create_dataset(X_train, y_train, window_size=window_size, predict_point=predict_point,\n",
        "                                      predict_single_timepoint=True,\n",
        "                                      pct_data_to_keep=1, return_traj_y=return_traj_y)\n",
        "    X_test, y_test = create_dataset(X_test, y_test, window_size=window_size, predict_point=predict_point,\n",
        "                                    predict_single_timepoint=True,\n",
        "                                    pct_data_to_keep=1, return_traj_y=return_traj_y)\n",
        "    plt.plot(X_train[:,0])\n",
        "    plt.plot(y_train)\n",
        "    plt.title('X_train and y_train - debugged error to before this point')\n",
        "    plt.show()\n",
        "    plt.plot(X_test[:,0])\n",
        "    plt.plot(y_test)\n",
        "    plt.title('X_test and y_test')\n",
        "    plt.show()\n",
        "    print(f'ytrain_mean = {torch.mean(y_train)}')\n",
        "    X_train = X_train.transpose(1, 2)\n",
        "    X_test = X_test.transpose(1, 2)\n",
        "\n",
        "    belief_model.eval()\n",
        "    with torch.no_grad():\n",
        "        observed_error_v = np.expand_dims(np.array(belief_training_buffer.observed_error_v),1)\n",
        "\n",
        "        train_plot = np.ones_like(observed_error_v) * 0\n",
        "        train_plot[window_size:train_size] = belief_model(X_train)\n",
        "        train_plot = np.squeeze(np.roll(train_plot,-predict_point))\n",
        "\n",
        "        # shift test predictions for plotting\n",
        "        test_plot = np.ones_like(observed_error_v) * 0\n",
        "        test_plot[train_size+window_size:len(observed_error_v)] = belief_model(X_test)\n",
        "        test_plot = np.squeeze(np.roll(test_plot,-predict_point))\n",
        "\n",
        "    plt.plot(np.squeeze(observed_error_v)*scale_factors*max_scale_mt_m) # this is the ground-truth error that we are trying to predict\n",
        "    plt.plot(train_plot*scale_factors*max_scale_mt_m, c='r')\n",
        "    plt.plot(test_plot*scale_factors*max_scale_mt_m, c='g')\n",
        "    plt.plot(scale_factors*max_scale_mt_m)\n",
        "    plt.title('normalized output')\n",
        "    plt.legend(['true waveform', 'train waveform prediction','test waveform prediction', 'scalefeature'])\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.abs(test_plot- np.squeeze(observed_error_v))*scale_factors*max_scale_mt_m, c='r')\n",
        "    plt.title('error (mT/m)')\n",
        "    plt.show()\n",
        "\n",
        "    mat_file_name = 'model_TCN_test_output_'+axis+'.mat'\n",
        "    print(f'Saving results to .mat file: {mat_file_name}')\n",
        "    test_cost = 0\n",
        "    savedict = {\"true_data\":np.squeeze(observed_error_v)*scale_factors*max_scale_mt_m,\n",
        "                \"predicted_train_data\":train_plot*scale_factors*max_scale_mt_m,\n",
        "                \"predicted_test_data\":test_plot*scale_factors*max_scale_mt_m,\n",
        "                \"input_data\":collected_observations,\n",
        "                \"min_test_loss\":test_cost}\n",
        "    sio.savemat(mat_file_name, savedict)\n"
      ],
      "metadata": {
        "id": "iIqjLf9El7hd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}